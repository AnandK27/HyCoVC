20:16:50 INFO - logger_setup: /home/kumarana/tmp/high_fidel/compress.py
20:16:50 INFO - logger_setup: /home/kumarana/tmp/high_fidel/compress.py
20:17:25 INFO - load_model: Loading model ...
20:17:25 INFO - load_model: MODEL TYPE: compression_gan
20:17:25 INFO - load_model: MODEL MODE: evaluation
20:17:25 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(6, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_7): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_8): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
    (hyperprior_entropy_model): HyperpriorEntropyModel(
      (distribution): HyperpriorDensity()
    )
    (prior_density): PriorDensity()
    (prior_entropy_model): PriorEntropyModel(
      (distribution): PriorDensity()
    )
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
20:17:25 INFO - load_model: Trainable parameters:
20:17:25 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 6, 7, 7])
20:17:25 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
20:17:25 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
20:17:25 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
20:17:25 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
20:17:25 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
20:17:25 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
20:17:25 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
20:17:25 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
20:17:25 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
20:17:25 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
20:17:25 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
20:17:25 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
20:17:25 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
20:17:25 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
20:17:25 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
20:17:25 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
20:17:25 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
20:17:25 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
20:17:25 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
20:17:25 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
20:17:25 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
20:17:25 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
20:17:25 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
20:17:25 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
20:17:25 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
20:17:25 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
20:17:25 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
20:17:25 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
20:17:25 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
20:17:25 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
20:17:25 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
20:17:25 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
20:17:25 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
20:17:25 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
20:17:25 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
20:17:25 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
20:17:25 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_7.conv1.weight - torch.Size([960, 960, 3, 3])
20:17:25 INFO - load_model: Generator.resblock_7.conv1.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.resblock_7.conv2.weight - torch.Size([960, 960, 3, 3])
20:17:25 INFO - load_model: Generator.resblock_7.conv2.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.resblock_7.norm1.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_7.norm1.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_7.norm2.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_7.norm2.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_8.conv1.weight - torch.Size([960, 960, 3, 3])
20:17:25 INFO - load_model: Generator.resblock_8.conv1.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.resblock_8.conv2.weight - torch.Size([960, 960, 3, 3])
20:17:25 INFO - load_model: Generator.resblock_8.conv2.bias - torch.Size([960])
20:17:25 INFO - load_model: Generator.resblock_8.norm1.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_8.norm1.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_8.norm2.gamma - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.resblock_8.norm2.beta - torch.Size([1, 960, 1, 1])
20:17:25 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
20:17:25 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
20:17:25 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
20:17:25 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
20:17:25 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
20:17:25 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
20:17:25 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
20:17:25 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
20:17:25 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
20:17:25 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
20:17:25 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
20:17:25 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
20:17:25 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
20:17:25 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
20:17:25 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
20:17:25 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
20:17:25 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
20:17:25 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
20:17:25 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
20:17:25 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
20:17:25 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
20:17:25 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
20:17:25 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
20:17:25 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
20:17:25 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
20:17:25 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
20:17:25 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
20:17:25 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
20:17:25 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
20:17:25 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
20:17:25 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
20:17:25 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
20:17:25 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
20:17:25 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
20:17:25 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
20:17:25 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
20:17:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
20:17:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
20:17:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
20:17:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
20:17:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
20:17:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
20:17:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
20:17:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
20:17:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
20:17:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
20:17:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
20:17:25 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
20:17:25 INFO - load_model: Hyperprior.prior_entropy_model.CDF - torch.Size([64, 1481])
20:17:25 INFO - load_model: Hyperprior.prior_entropy_model.CDF_offset - torch.Size([64])
20:17:25 INFO - load_model: Hyperprior.prior_entropy_model.CDF_length - torch.Size([64])
20:17:25 INFO - load_model: Number of trainable parameters: 181484483
20:17:25 INFO - load_model: Estimated model size (under fp32): 725.938 MB
20:17:25 INFO - load_model: Model init 32.796s
20:17:25 INFO - compress_and_decompress: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/vimeo_compression_gan_2022_06_22_20_32/checkpoints', 'crop_size': 256, 'dataset': 'vimeo', 'dataset_path': '/data/videocoding/dnnvc/datasets/Vimeo-90k/tmp/vimeo_septuplet/sequences/', 'discriminator_steps': 1, 'figures_save': 'experiments/vimeo_compression_gan_2022_06_22_20_32/figures', 'force_set_gpu': False, 'gan_loss_type': 'non_saturating', 'gpu': 0, 'ignore_schedule': True, 'image_dims': (6, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 0.5, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_channels_DLMM': 64, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'mixture_components': 4, 'model_mode': 'training', 'model_type': 'compression_gan', 'multigpu': False, 'n_data': 387672, 'n_epochs': 8, 'n_residual_blocks': 9, 'n_steps': 200000, 'name': 'vimeo_compression_gan_2022_06_22_20_32', 'noise_dim': 32, 'normalize_input_image': False, 'regime': 'high', 'sample_noise': False, 'save': False, 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/vimeo_compression_gan_2022_06_22_20_32', 'storage_save': 'experiments/vimeo_compression_gan_2022_06_22_20_32/storage', 'target_rate': 0.45, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/vimeo_compression_gan_2022_06_22_20_32/tensorboard', 'timestamp': '2022_06_23_19:16', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': True, 'warmstart_ckpt': 'experiments/vimeo_compression_2022_06_22_14_49/checkpoints/vimeo_compression_2022_06_22_14_49_epoch1_idx64612_2022_06_22_19:38.pt', 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(ckpt_path='/home/kumarana/tmp/high_fidel/experiments/vimeo_high_compression_gan_2022_06_22_200_32/checkpoints/vimeo_compression_gan_2022_06_22_20_32_epoch6_idx200001_2022_06_23_19:16.pt', image_dir='uvg', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=True, img_save=False)>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(ckpt_path='/home/kumarana/tmp/high_fidel/experiments/vimeo_high_compression_gan_2022_06_22_200_32/checkpoints/vimeo_compression_gan_2022_06_22_20_32_epoch6_idx200001_2022_06_23_19:16.pt', image_dir='uvg', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=True, img_save=False)>, 'ckpt_path': '/home/kumarana/tmp/high_fidel/experiments/vimeo_high_compression_gan_2022_06_22_200_32/checkpoints/vimeo_compression_gan_2022_06_22_20_32_epoch6_idx200001_2022_06_23_19:16.pt', 'image_dir': 'uvg', 'img_save': False, 'metrics': True, 'output_dir': 'data/reconstructions', 'reconstruct': True}
20:17:25 INFO - compress_and_decompress: Building hyperprior probability tables...
 Starting compression...
20:17:26 INFO - load_model: Loading model ...
20:17:26 INFO - load_model: MODEL TYPE: compression_gan
20:17:26 INFO - load_model: MODEL MODE: evaluation
20:17:26 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(6, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_7): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_8): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
    (hyperprior_entropy_model): HyperpriorEntropyModel(
      (distribution): HyperpriorDensity()
    )
    (prior_density): PriorDensity()
    (prior_entropy_model): PriorEntropyModel(
      (distribution): PriorDensity()
    )
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
20:17:26 INFO - load_model: Trainable parameters:
20:17:26 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 6, 7, 7])
20:17:26 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
20:17:26 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
20:17:26 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
20:17:26 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
20:17:26 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
20:17:26 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
20:17:26 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
20:17:26 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
20:17:26 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
20:17:26 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
20:17:26 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
20:17:26 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
20:17:26 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
20:17:26 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
20:17:26 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
20:17:26 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
20:17:26 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
20:17:26 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
20:17:26 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
20:17:26 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
20:17:26 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
20:17:26 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
20:17:26 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
20:17:26 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
20:17:26 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
20:17:26 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
20:17:26 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
20:17:26 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
20:17:26 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
20:17:26 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
20:17:26 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
20:17:26 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
20:17:26 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
20:17:26 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
20:17:26 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
20:17:26 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
20:17:26 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_7.conv1.weight - torch.Size([960, 960, 3, 3])
20:17:26 INFO - load_model: Generator.resblock_7.conv1.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.resblock_7.conv2.weight - torch.Size([960, 960, 3, 3])
20:17:26 INFO - load_model: Generator.resblock_7.conv2.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.resblock_7.norm1.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_7.norm1.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_7.norm2.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_7.norm2.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_8.conv1.weight - torch.Size([960, 960, 3, 3])
20:17:26 INFO - load_model: Generator.resblock_8.conv1.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.resblock_8.conv2.weight - torch.Size([960, 960, 3, 3])
20:17:26 INFO - load_model: Generator.resblock_8.conv2.bias - torch.Size([960])
20:17:26 INFO - load_model: Generator.resblock_8.norm1.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_8.norm1.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_8.norm2.gamma - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.resblock_8.norm2.beta - torch.Size([1, 960, 1, 1])
20:17:26 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
20:17:26 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
20:17:26 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
20:17:26 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
20:17:26 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
20:17:26 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
20:17:26 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
20:17:26 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
20:17:26 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
20:17:26 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
20:17:26 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
20:17:26 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
20:17:26 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
20:17:26 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
20:17:26 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
20:17:26 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
20:17:26 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
20:17:26 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
20:17:26 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
20:17:26 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
20:17:26 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
20:17:26 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
20:17:26 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
20:17:26 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
20:17:26 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
20:17:26 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
20:17:26 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
20:17:26 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
20:17:26 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
20:17:26 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
20:17:26 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
20:17:26 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
20:17:26 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
20:17:26 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
20:17:26 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
20:17:26 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
20:17:26 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
20:17:26 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
20:17:26 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
20:17:26 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
20:17:26 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
20:17:26 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
20:17:26 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
20:17:26 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
20:17:26 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
20:17:26 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
20:17:26 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
20:17:26 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
20:17:26 INFO - load_model: Hyperprior.prior_entropy_model.CDF - torch.Size([64, 1481])
20:17:26 INFO - load_model: Hyperprior.prior_entropy_model.CDF_offset - torch.Size([64])
20:17:26 INFO - load_model: Hyperprior.prior_entropy_model.CDF_length - torch.Size([64])
20:17:26 INFO - load_model: Number of trainable parameters: 181484483
20:17:26 INFO - load_model: Estimated model size (under fp32): 725.938 MB
20:17:26 INFO - load_model: Model init 36.379s
20:17:26 INFO - compress_and_decompress: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/vimeo_med_compression_gan_2022_06_27_17_25/checkpoints', 'crop_size': 256, 'dataset': 'vimeo', 'dataset_path': '/data/videocoding/dnnvc/datasets/Vimeo-90k/tmp/vimeo_septuplet/sequences/', 'discriminator_steps': 1, 'figures_save': 'experiments/vimeo_med_compression_gan_2022_06_27_17_25/figures', 'force_set_gpu': False, 'gan_loss_type': 'non_saturating', 'gpu': 0, 'ignore_schedule': True, 'image_dims': (6, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 1, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_channels_DLMM': 64, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'mixture_components': 4, 'model_mode': 'training', 'model_type': 'compression_gan', 'multigpu': False, 'n_data': 387672, 'n_epochs': 8, 'n_residual_blocks': 9, 'n_steps': 50000, 'name': 'vimeo_med_compression_gan_2022_06_27_17_25', 'noise_dim': 32, 'normalize_input_image': False, 'regime': 'med', 'sample_noise': False, 'save': False, 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/vimeo_med_compression_gan_2022_06_27_17_25', 'storage_save': 'experiments/vimeo_med_compression_gan_2022_06_27_17_25/storage', 'target_rate': 0.3, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/vimeo_med_compression_gan_2022_06_27_17_25/tensorboard', 'timestamp': '2022_06_27_22:37', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': True, 'warmstart_ckpt': 'experiments/vimeo_compression_gan_2022_06_26_20_50/checkpoints/vimeo_compression_gan_2022_06_26_20_50_epoch5_idx161530_2022_06_27_13:23.pt', 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(ckpt_path='experiments/vimeo_med_compression_gan_2022_06_27_17_25/checkpoints/vimeo_med_compression_gan_2022_06_27_17_25_epoch1_idx50001_2022_06_27_22:37.pt', image_dir='uvg', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=True, img_save=False)>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(ckpt_path='experiments/vimeo_med_compression_gan_2022_06_27_17_25/checkpoints/vimeo_med_compression_gan_2022_06_27_17_25_epoch1_idx50001_2022_06_27_22:37.pt', image_dir='uvg', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=True, img_save=False)>, 'ckpt_path': 'experiments/vimeo_med_compression_gan_2022_06_27_17_25/checkpoints/vimeo_med_compression_gan_2022_06_27_17_25_epoch1_idx50001_2022_06_27_22:37.pt', 'image_dir': 'uvg', 'img_save': False, 'metrics': True, 'output_dir': 'data/reconstructions', 'reconstruct': True}
20:17:26 INFO - compress_and_decompress: Building hyperprior probability tables...
20:17:31 INFO - compress_and_decompress: All tables built.
20:17:31 INFO - compress_and_decompress: Starting compression...
10:30:56 INFO - logger_setup: /home/kumarana/tmp/high_fidel/compress.py
10:31:58 INFO - logger_setup: /home/kumarana/tmp/high_fidel/compress.py
10:31:49 INFO - load_model: Loading model ...
10:31:49 INFO - load_model: MODEL TYPE: compression_gan
10:31:49 INFO - load_model: MODEL MODE: evaluation
10:31:49 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(6, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_7): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_8): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
    (hyperprior_entropy_model): HyperpriorEntropyModel(
      (distribution): HyperpriorDensity()
    )
    (prior_density): PriorDensity()
    (prior_entropy_model): PriorEntropyModel(
      (distribution): PriorDensity()
    )
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
10:31:49 INFO - load_model: Trainable parameters:
10:31:49 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 6, 7, 7])
10:31:49 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
10:31:49 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
10:31:49 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
10:31:49 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
10:31:49 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
10:31:49 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
10:31:49 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
10:31:49 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
10:31:49 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
10:31:49 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
10:31:49 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
10:31:49 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
10:31:49 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
10:31:49 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
10:31:49 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
10:31:49 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
10:31:49 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
10:31:49 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
10:31:49 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
10:31:49 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
10:31:49 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
10:31:49 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
10:31:49 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
10:31:49 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
10:31:49 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
10:31:49 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
10:31:49 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
10:31:49 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
10:31:49 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
10:31:49 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
10:31:49 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
10:31:49 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
10:31:49 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
10:31:49 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
10:31:49 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
10:31:49 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
10:31:49 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_7.conv1.weight - torch.Size([960, 960, 3, 3])
10:31:49 INFO - load_model: Generator.resblock_7.conv1.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.resblock_7.conv2.weight - torch.Size([960, 960, 3, 3])
10:31:49 INFO - load_model: Generator.resblock_7.conv2.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.resblock_7.norm1.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_7.norm1.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_7.norm2.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_7.norm2.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_8.conv1.weight - torch.Size([960, 960, 3, 3])
10:31:49 INFO - load_model: Generator.resblock_8.conv1.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.resblock_8.conv2.weight - torch.Size([960, 960, 3, 3])
10:31:49 INFO - load_model: Generator.resblock_8.conv2.bias - torch.Size([960])
10:31:49 INFO - load_model: Generator.resblock_8.norm1.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_8.norm1.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_8.norm2.gamma - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.resblock_8.norm2.beta - torch.Size([1, 960, 1, 1])
10:31:49 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
10:31:49 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
10:31:49 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
10:31:49 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
10:31:49 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
10:31:49 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
10:31:49 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
10:31:49 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
10:31:49 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
10:31:49 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
10:31:49 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
10:31:49 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
10:31:49 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
10:31:49 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
10:31:49 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
10:31:49 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
10:31:49 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
10:31:49 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
10:31:49 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
10:31:49 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
10:31:49 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
10:31:49 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
10:31:49 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
10:31:49 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
10:31:49 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
10:31:49 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
10:31:49 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
10:31:49 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
10:31:49 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
10:31:49 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
10:31:49 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
10:31:49 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
10:31:49 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
10:31:49 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
10:31:49 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
10:31:49 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
10:31:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
10:31:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
10:31:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
10:31:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
10:31:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
10:31:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
10:31:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
10:31:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
10:31:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
10:31:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
10:31:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
10:31:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
10:31:49 INFO - load_model: Hyperprior.prior_entropy_model.CDF - torch.Size([64, 1481])
10:31:49 INFO - load_model: Hyperprior.prior_entropy_model.CDF_offset - torch.Size([64])
10:31:49 INFO - load_model: Hyperprior.prior_entropy_model.CDF_length - torch.Size([64])
10:31:49 INFO - load_model: Number of trainable parameters: 181484483
10:31:49 INFO - load_model: Estimated model size (under fp32): 725.938 MB
10:31:49 INFO - load_model: Model init 52.645s
10:31:49 INFO - compress_and_decompress: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/vimeo_compression_gan_2022_06_22_20_32/checkpoints', 'crop_size': 256, 'dataset': 'vimeo', 'dataset_path': '/data/videocoding/dnnvc/datasets/Vimeo-90k/tmp/vimeo_septuplet/sequences/', 'discriminator_steps': 1, 'figures_save': 'experiments/vimeo_compression_gan_2022_06_22_20_32/figures', 'force_set_gpu': False, 'gan_loss_type': 'non_saturating', 'gpu': 0, 'ignore_schedule': True, 'image_dims': (6, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 0.5, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_channels_DLMM': 64, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'mixture_components': 4, 'model_mode': 'training', 'model_type': 'compression_gan', 'multigpu': False, 'n_data': 387672, 'n_epochs': 8, 'n_residual_blocks': 9, 'n_steps': 200000, 'name': 'vimeo_compression_gan_2022_06_22_20_32', 'noise_dim': 32, 'normalize_input_image': False, 'regime': 'high', 'sample_noise': False, 'save': False, 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/vimeo_compression_gan_2022_06_22_20_32', 'storage_save': 'experiments/vimeo_compression_gan_2022_06_22_20_32/storage', 'target_rate': 0.45, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/vimeo_compression_gan_2022_06_22_20_32/tensorboard', 'timestamp': '2022_06_23_19:16', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': True, 'warmstart_ckpt': 'experiments/vimeo_compression_2022_06_22_14_49/checkpoints/vimeo_compression_2022_06_22_14_49_epoch1_idx64612_2022_06_22_19:38.pt', 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(ckpt_path='/home/kumarana/tmp/high_fidel/experiments/vimeo_high_compression_gan_2022_06_22_200_32/checkpoints/vimeo_compression_gan_2022_06_22_20_32_epoch6_idx200001_2022_06_23_19:16.pt', image_dir='uvg', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=True, img_save=False)>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(ckpt_path='/home/kumarana/tmp/high_fidel/experiments/vimeo_high_compression_gan_2022_06_22_200_32/checkpoints/vimeo_compression_gan_2022_06_22_20_32_epoch6_idx200001_2022_06_23_19:16.pt', image_dir='uvg', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=True, img_save=False)>, 'ckpt_path': '/home/kumarana/tmp/high_fidel/experiments/vimeo_high_compression_gan_2022_06_22_200_32/checkpoints/vimeo_compression_gan_2022_06_22_20_32_epoch6_idx200001_2022_06_23_19:16.pt', 'image_dir': 'uvg', 'img_save': False, 'metrics': True, 'output_dir': 'data/reconstructions', 'reconstruct': True}
10:31:49 INFO - compress_and_decompress: Building hyperprior probability tables...
10:32:08 INFO - compress_and_decompress: All tables built.
10:32:08 INFO - compress_and_decompress: Starting compression...
odel: MODEL MODE: evaluation
10:33:05 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(6, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_7): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_8): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
    (hyperprior_entropy_model): HyperpriorEntropyModel(
      (distribution): HyperpriorDensity()
    )
    (prior_density): PriorDensity()
    (prior_entropy_model): PriorEntropyModel(
      (distribution): PriorDensity()
    )
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
10:33:05 INFO - load_model: Trainable parameters:
10:33:05 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 6, 7, 7])
10:33:05 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
10:33:05 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
10:33:05 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
10:33:05 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
10:33:05 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
10:33:05 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
10:33:05 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
10:33:05 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
10:33:05 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
10:33:05 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
10:33:05 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
10:33:05 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
10:33:05 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
10:33:05 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
10:33:05 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
10:33:05 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
10:33:05 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
10:33:05 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
10:33:05 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
10:33:05 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
10:33:05 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
10:33:05 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
10:33:05 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
10:33:05 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
10:33:05 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
10:33:05 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
10:33:05 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
10:33:05 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
10:33:05 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
10:33:05 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
10:33:05 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
10:33:05 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
10:33:05 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
10:33:05 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
10:33:05 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
10:33:05 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
10:33:05 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_7.conv1.weight - torch.Size([960, 960, 3, 3])
10:33:05 INFO - load_model: Generator.resblock_7.conv1.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.resblock_7.conv2.weight - torch.Size([960, 960, 3, 3])
10:33:05 INFO - load_model: Generator.resblock_7.conv2.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.resblock_7.norm1.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_7.norm1.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_7.norm2.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_7.norm2.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_8.conv1.weight - torch.Size([960, 960, 3, 3])
10:33:05 INFO - load_model: Generator.resblock_8.conv1.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.resblock_8.conv2.weight - torch.Size([960, 960, 3, 3])
10:33:05 INFO - load_model: Generator.resblock_8.conv2.bias - torch.Size([960])
10:33:05 INFO - load_model: Generator.resblock_8.norm1.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_8.norm1.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_8.norm2.gamma - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.resblock_8.norm2.beta - torch.Size([1, 960, 1, 1])
10:33:05 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
10:33:05 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
10:33:05 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
10:33:05 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
10:33:05 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
10:33:05 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
10:33:05 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
10:33:05 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
10:33:05 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
10:33:05 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
10:33:05 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
10:33:05 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
10:33:05 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
10:33:05 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
10:33:05 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
10:33:05 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
10:33:05 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
10:33:05 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
10:33:05 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
10:33:05 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
10:33:05 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
10:33:05 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
10:33:05 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
10:33:05 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
10:33:05 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
10:33:05 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
10:33:05 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
10:33:05 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
10:33:05 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
10:33:05 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
10:33:05 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
10:33:05 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
10:33:05 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
10:33:05 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
10:33:05 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
10:33:05 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
10:33:05 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
10:33:05 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
10:33:05 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
10:33:05 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
10:33:05 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
10:33:05 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
10:33:05 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
10:33:05 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
10:33:05 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
10:33:05 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
10:33:05 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
10:33:05 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
10:33:05 INFO - load_model: Hyperprior.prior_entropy_model.CDF - torch.Size([64, 1481])
10:33:05 INFO - load_model: Hyperprior.prior_entropy_model.CDF_offset - torch.Size([64])
10:33:05 INFO - load_model: Hyperprior.prior_entropy_model.CDF_length - torch.Size([64])
10:33:05 INFO - load_model: Number of trainable parameters: 181484483
10:33:05 INFO - load_model: Estimated model size (under fp32): 725.938 MB
10:33:05 INFO - load_model: Model init 66.680s
10:33:05 INFO - compress_and_decompress: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/vimeo_med_compression_gan_2022_06_27_17_25/checkpoints', 'crop_size': 256, 'dataset': 'vimeo', 'dataset_path': '/data/videocoding/dnnvc/datasets/Vimeo-90k/tmp/vimeo_septuplet/sequences/', 'discriminator_steps': 1, 'figures_save': 'experiments/vimeo_med_compression_gan_2022_06_27_17_25/figures', 'force_set_gpu': False, 'gan_loss_type': 'non_saturating', 'gpu': 0, 'ignore_schedule': True, 'image_dims': (6, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 1, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_channels_DLMM': 64, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'mixture_components': 4, 'model_mode': 'training', 'model_type': 'compression_gan', 'multigpu': False, 'n_data': 387672, 'n_epochs': 8, 'n_residual_blocks': 9, 'n_steps': 50000, 'name': 'vimeo_med_compression_gan_2022_06_27_17_25', 'noise_dim': 32, 'normalize_input_image': False, 'regime': 'med', 'sample_noise': False, 'save': False, 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/vimeo_med_compression_gan_2022_06_27_17_25', 'storage_save': 'experiments/vimeo_med_compression_gan_2022_06_27_17_25/storage', 'target_rate': 0.3, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/vimeo_med_compression_gan_2022_06_27_17_25/tensorboard', 'timestamp': '2022_06_27_22:37', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': True, 'warmstart_ckpt': 'experiments/vimeo_compression_gan_2022_06_26_20_50/checkpoints/vimeo_compression_gan_2022_06_26_20_50_epoch5_idx161530_2022_06_27_13:23.pt', 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(ckpt_path='experiments/vimeo_med_compression_gan_2022_06_27_17_25/checkpoints/vimeo_med_compression_gan_2022_06_27_17_25_epoch1_idx50001_2022_06_27_22:37.pt', image_dir='uvg', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=True, img_save=False)>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(ckpt_path='experiments/vimeo_med_compression_gan_2022_06_27_17_25/checkpoints/vimeo_med_compression_gan_2022_06_27_17_25_epoch1_idx50001_2022_06_27_22:37.pt', image_dir='uvg', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=True, img_save=False)>, 'ckpt_path': 'experiments/vimeo_med_compression_gan_2022_06_27_17_25/checkpoints/vimeo_med_compression_gan_2022_06_27_17_25_epoch1_idx50001_2022_06_27_22:37.pt', 'image_dir': 'uvg', 'img_save': False, 'metrics': True, 'output_dir': 'data/reconstructions', 'reconstruct': True}
10:33:05 INFO - compress_and_decompress: Building hyperprior probability tables...
10:33:09 INFO - compress_and_decompress: All tables built.
10:33:09 INFO - compress_and_decompress: All tables built.
10:33:09 INFO - compress_and_decompress: Starting compression...
10:33:09 INFO - compress_and_decompress: Starting compression...
11:58:07 INFO - logger_setup: /home/kumarana/tmp/high_fidel/compress.py
11:58:13 INFO - load_model: Loading model ...
11:58:13 INFO - load_model: MODEL TYPE: compression_gan
11:58:13 INFO - load_model: MODEL MODE: evaluation
11:58:13 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(6, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_7): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_8): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
    (hyperprior_entropy_model): HyperpriorEntropyModel(
      (distribution): HyperpriorDensity()
    )
    (prior_density): PriorDensity()
    (prior_entropy_model): PriorEntropyModel(
      (distribution): PriorDensity()
    )
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
11:58:13 INFO - load_model: Trainable parameters:
11:58:13 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 6, 7, 7])
11:58:13 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
11:58:13 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
11:58:13 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
11:58:13 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
11:58:13 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
11:58:13 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
11:58:13 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
11:58:13 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
11:58:13 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
11:58:13 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
11:58:13 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
11:58:13 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
11:58:13 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
11:58:13 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
11:58:13 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
11:58:13 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
11:58:13 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
11:58:13 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
11:58:13 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
11:58:13 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
11:58:13 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
11:58:13 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
11:58:13 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
11:58:13 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
11:58:13 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
11:58:13 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
11:58:13 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
11:58:13 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
11:58:13 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
11:58:13 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
11:58:13 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
11:58:13 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
11:58:13 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
11:58:13 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
11:58:13 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
11:58:13 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
11:58:13 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_7.conv1.weight - torch.Size([960, 960, 3, 3])
11:58:13 INFO - load_model: Generator.resblock_7.conv1.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.resblock_7.conv2.weight - torch.Size([960, 960, 3, 3])
11:58:13 INFO - load_model: Generator.resblock_7.conv2.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.resblock_7.norm1.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_7.norm1.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_7.norm2.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_7.norm2.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_8.conv1.weight - torch.Size([960, 960, 3, 3])
11:58:13 INFO - load_model: Generator.resblock_8.conv1.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.resblock_8.conv2.weight - torch.Size([960, 960, 3, 3])
11:58:13 INFO - load_model: Generator.resblock_8.conv2.bias - torch.Size([960])
11:58:13 INFO - load_model: Generator.resblock_8.norm1.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_8.norm1.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_8.norm2.gamma - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.resblock_8.norm2.beta - torch.Size([1, 960, 1, 1])
11:58:13 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
11:58:13 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
11:58:13 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
11:58:13 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
11:58:13 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
11:58:13 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
11:58:13 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
11:58:13 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
11:58:13 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
11:58:13 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
11:58:13 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
11:58:13 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
11:58:13 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
11:58:13 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
11:58:13 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
11:58:13 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
11:58:13 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
11:58:13 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
11:58:13 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
11:58:13 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
11:58:13 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
11:58:13 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
11:58:13 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
11:58:13 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
11:58:13 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
11:58:13 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
11:58:13 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
11:58:13 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
11:58:13 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
11:58:13 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
11:58:13 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
11:58:13 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
11:58:13 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
11:58:13 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
11:58:13 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
11:58:13 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
11:58:13 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
11:58:13 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
11:58:13 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
11:58:13 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
11:58:13 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
11:58:13 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
11:58:13 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
11:58:13 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
11:58:13 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
11:58:13 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
11:58:13 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
11:58:13 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
11:58:13 INFO - load_model: Hyperprior.prior_entropy_model.CDF - torch.Size([64, 1481])
11:58:13 INFO - load_model: Hyperprior.prior_entropy_model.CDF_offset - torch.Size([64])
11:58:13 INFO - load_model: Hyperprior.prior_entropy_model.CDF_length - torch.Size([64])
11:58:13 INFO - load_model: Number of trainable parameters: 181484483
11:58:13 INFO - load_model: Estimated model size (under fp32): 725.938 MB
11:58:13 INFO - load_model: Model init 5.546s
11:58:13 INFO - compress_and_decompress: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/vimeo_compression_gan_2022_06_20_17_40/checkpoints', 'crop_size': 256, 'dataset': 'vimeo', 'dataset_path': '/data/videocoding/dnnvc/datasets/Vimeo-90k/tmp/vimeo_septuplet/sequences/', 'discriminator_steps': 1, 'figures_save': 'experiments/vimeo_compression_gan_2022_06_20_17_40/figures', 'force_set_gpu': False, 'gan_loss_type': 'non_saturating', 'gpu': 0, 'ignore_schedule': True, 'image_dims': (6, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 2, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_channels_DLMM': 64, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'mixture_components': 4, 'model_mode': 'training', 'model_type': 'compression_gan', 'multigpu': False, 'n_data': 387672, 'n_epochs': 8, 'n_residual_blocks': 9, 'n_steps': 200000, 'name': 'vimeo_compression_gan_2022_06_20_17_40', 'noise_dim': 32, 'normalize_input_image': False, 'regime': 'low', 'sample_noise': False, 'save': False, 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/vimeo_compression_gan_2022_06_20_17_40', 'storage_save': 'experiments/vimeo_compression_gan_2022_06_20_17_40/storage', 'target_rate': 0.14, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/vimeo_compression_gan_2022_06_20_17_40/tensorboard', 'timestamp': '2022_06_21_14:22', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': True, 'warmstart_ckpt': 'experiments/vimeo_compression_2022_06_20_12_28/checkpoints/vimeo_compression_2022_06_20_12_28_epoch1_idx64612_2022_06_20_17:14.pt', 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(ckpt_path='experiments/vimeo_low_compression_gan_2022_06_20_17_40/checkpoints/vimeo_compression_gan_2022_06_20_17_40_epoch6_idx200001_2022_06_21_14:22.pt', image_dir='uvg', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=True, img_save=False)>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(ckpt_path='experiments/vimeo_low_compression_gan_2022_06_20_17_40/checkpoints/vimeo_compression_gan_2022_06_20_17_40_epoch6_idx200001_2022_06_21_14:22.pt', image_dir='uvg', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=True, img_save=False)>, 'ckpt_path': 'experiments/vimeo_low_compression_gan_2022_06_20_17_40/checkpoints/vimeo_compression_gan_2022_06_20_17_40_epoch6_idx200001_2022_06_21_14:22.pt', 'image_dir': 'uvg', 'img_save': False, 'metrics': True, 'output_dir': 'data/reconstructions', 'reconstruct': True}
11:58:13 INFO - compress_and_decompress: Building hyperprior probability tables...
11:58:18 INFO - compress_and_decompress: All tables built.
11:58:18 INFO - compress_and_decompress: Starting compression...
12:02:37 INFO - logger_setup: /home/kumarana/tmp/high_fidel/compress.py
12:02:42 INFO - load_model: Loading model ...
12:02:42 INFO - load_model: MODEL TYPE: compression_gan
12:02:42 INFO - load_model: MODEL MODE: evaluation
12:02:42 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(6, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_7): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_8): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
    (hyperprior_entropy_model): HyperpriorEntropyModel(
      (distribution): HyperpriorDensity()
    )
    (prior_density): PriorDensity()
    (prior_entropy_model): PriorEntropyModel(
      (distribution): PriorDensity()
    )
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
12:02:42 INFO - load_model: Trainable parameters:
12:02:42 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 6, 7, 7])
12:02:42 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
12:02:42 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
12:02:42 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
12:02:42 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
12:02:42 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
12:02:42 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
12:02:42 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
12:02:42 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
12:02:42 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
12:02:42 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
12:02:42 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
12:02:42 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
12:02:42 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
12:02:42 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
12:02:42 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
12:02:42 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
12:02:42 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
12:02:42 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
12:02:42 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
12:02:42 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
12:02:42 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
12:02:42 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
12:02:42 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
12:02:42 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
12:02:42 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
12:02:42 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
12:02:42 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
12:02:42 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
12:02:42 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
12:02:42 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
12:02:42 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
12:02:42 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
12:02:42 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
12:02:42 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
12:02:42 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
12:02:42 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
12:02:42 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_7.conv1.weight - torch.Size([960, 960, 3, 3])
12:02:42 INFO - load_model: Generator.resblock_7.conv1.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.resblock_7.conv2.weight - torch.Size([960, 960, 3, 3])
12:02:42 INFO - load_model: Generator.resblock_7.conv2.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.resblock_7.norm1.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_7.norm1.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_7.norm2.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_7.norm2.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_8.conv1.weight - torch.Size([960, 960, 3, 3])
12:02:42 INFO - load_model: Generator.resblock_8.conv1.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.resblock_8.conv2.weight - torch.Size([960, 960, 3, 3])
12:02:42 INFO - load_model: Generator.resblock_8.conv2.bias - torch.Size([960])
12:02:42 INFO - load_model: Generator.resblock_8.norm1.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_8.norm1.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_8.norm2.gamma - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.resblock_8.norm2.beta - torch.Size([1, 960, 1, 1])
12:02:42 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
12:02:42 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
12:02:42 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
12:02:42 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
12:02:42 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
12:02:42 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
12:02:42 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
12:02:42 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
12:02:42 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
12:02:42 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
12:02:42 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
12:02:42 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
12:02:42 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
12:02:42 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
12:02:42 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
12:02:42 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
12:02:42 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
12:02:42 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
12:02:42 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
12:02:42 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
12:02:42 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
12:02:42 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
12:02:42 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
12:02:42 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
12:02:42 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
12:02:42 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
12:02:42 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
12:02:42 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
12:02:42 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
12:02:42 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
12:02:42 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
12:02:42 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
12:02:42 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
12:02:42 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
12:02:42 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
12:02:42 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
12:02:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
12:02:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
12:02:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
12:02:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
12:02:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
12:02:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
12:02:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
12:02:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
12:02:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
12:02:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
12:02:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
12:02:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
12:02:42 INFO - load_model: Hyperprior.prior_entropy_model.CDF - torch.Size([64, 1481])
12:02:42 INFO - load_model: Hyperprior.prior_entropy_model.CDF_offset - torch.Size([64])
12:02:42 INFO - load_model: Hyperprior.prior_entropy_model.CDF_length - torch.Size([64])
12:02:42 INFO - load_model: Number of trainable parameters: 181484483
12:02:42 INFO - load_model: Estimated model size (under fp32): 725.938 MB
12:02:42 INFO - load_model: Model init 5.312s
12:02:42 INFO - compress_and_decompress: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/vimeo_compression_gan_2022_06_20_17_40/checkpoints', 'crop_size': 256, 'dataset': 'vimeo', 'dataset_path': '/data/videocoding/dnnvc/datasets/Vimeo-90k/tmp/vimeo_septuplet/sequences/', 'discriminator_steps': 1, 'figures_save': 'experiments/vimeo_compression_gan_2022_06_20_17_40/figures', 'force_set_gpu': False, 'gan_loss_type': 'non_saturating', 'gpu': 0, 'ignore_schedule': True, 'image_dims': (6, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 2, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_channels_DLMM': 64, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'mixture_components': 4, 'model_mode': 'training', 'model_type': 'compression_gan', 'multigpu': False, 'n_data': 387672, 'n_epochs': 8, 'n_residual_blocks': 9, 'n_steps': 200000, 'name': 'vimeo_compression_gan_2022_06_20_17_40', 'noise_dim': 32, 'normalize_input_image': False, 'regime': 'low', 'sample_noise': False, 'save': False, 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/vimeo_compression_gan_2022_06_20_17_40', 'storage_save': 'experiments/vimeo_compression_gan_2022_06_20_17_40/storage', 'target_rate': 0.14, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/vimeo_compression_gan_2022_06_20_17_40/tensorboard', 'timestamp': '2022_06_21_14:22', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': True, 'warmstart_ckpt': 'experiments/vimeo_compression_2022_06_20_12_28/checkpoints/vimeo_compression_2022_06_20_12_28_epoch1_idx64612_2022_06_20_17:14.pt', 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(ckpt_path='experiments/vimeo_low_compression_gan_2022_06_20_17_40/checkpoints/vimeo_compression_gan_2022_06_20_17_40_epoch6_idx200001_2022_06_21_14:22.pt', image_dir='uvg', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=True, img_save=False)>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(ckpt_path='experiments/vimeo_low_compression_gan_2022_06_20_17_40/checkpoints/vimeo_compression_gan_2022_06_20_17_40_epoch6_idx200001_2022_06_21_14:22.pt', image_dir='uvg', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=True, img_save=False)>, 'ckpt_path': 'experiments/vimeo_low_compression_gan_2022_06_20_17_40/checkpoints/vimeo_compression_gan_2022_06_20_17_40_epoch6_idx200001_2022_06_21_14:22.pt', 'image_dir': 'uvg', 'img_save': False, 'metrics': True, 'output_dir': 'data/reconstructions', 'reconstruct': True}
12:02:42 INFO - compress_and_decompress: Building hyperprior probability tables...
12:02:47 INFO - compress_and_decompress: All tables built.
12:02:47 INFO - compress_and_decompress: Starting compression...
12:42:23 INFO - compress_and_decompress: Complete. Reconstructions saved to data/reconstructions. Output statistics saved to data/reconstructions/compression_metrics_low.h5
12:42:23 INFO - compress_and_decompress: Time elapsed: 2376.721 s
12:42:23 INFO - compress_and_decompress: Rate: 1.638 Images / s:
13:48:30 INFO - logger_setup: /home/kumarana/tmp/high_fidel/compress.py
13:48:42 INFO - logger_setup: /home/kumarana/tmp/high_fidel/compress.py
13:50:45 INFO - logger_setup: /home/kumarana/tmp/high_fidel/compress.py
13:50:49 INFO - load_model: Loading model ...
13:50:49 INFO - load_model: MODEL TYPE: compression_gan
13:50:49 INFO - load_model: MODEL MODE: evaluation
13:50:49 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(6, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_7): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_8): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
    (hyperprior_entropy_model): HyperpriorEntropyModel(
      (distribution): HyperpriorDensity()
    )
    (prior_density): PriorDensity()
    (prior_entropy_model): PriorEntropyModel(
      (distribution): PriorDensity()
    )
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
13:50:49 INFO - load_model: Trainable parameters:
13:50:49 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 6, 7, 7])
13:50:49 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
13:50:49 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
13:50:49 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
13:50:49 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
13:50:49 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
13:50:49 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
13:50:49 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
13:50:49 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
13:50:49 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
13:50:49 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
13:50:49 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
13:50:49 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
13:50:49 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
13:50:49 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
13:50:49 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
13:50:49 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
13:50:49 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
13:50:49 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
13:50:49 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
13:50:49 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
13:50:49 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
13:50:49 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
13:50:49 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
13:50:49 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
13:50:49 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
13:50:49 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
13:50:49 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
13:50:49 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
13:50:49 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
13:50:49 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
13:50:49 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
13:50:49 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
13:50:49 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
13:50:49 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
13:50:49 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
13:50:49 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
13:50:49 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_7.conv1.weight - torch.Size([960, 960, 3, 3])
13:50:49 INFO - load_model: Generator.resblock_7.conv1.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.resblock_7.conv2.weight - torch.Size([960, 960, 3, 3])
13:50:49 INFO - load_model: Generator.resblock_7.conv2.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.resblock_7.norm1.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_7.norm1.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_7.norm2.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_7.norm2.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_8.conv1.weight - torch.Size([960, 960, 3, 3])
13:50:49 INFO - load_model: Generator.resblock_8.conv1.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.resblock_8.conv2.weight - torch.Size([960, 960, 3, 3])
13:50:49 INFO - load_model: Generator.resblock_8.conv2.bias - torch.Size([960])
13:50:49 INFO - load_model: Generator.resblock_8.norm1.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_8.norm1.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_8.norm2.gamma - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.resblock_8.norm2.beta - torch.Size([1, 960, 1, 1])
13:50:49 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
13:50:49 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
13:50:49 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
13:50:49 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
13:50:49 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
13:50:49 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
13:50:49 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
13:50:49 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
13:50:49 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
13:50:49 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
13:50:49 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
13:50:49 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
13:50:49 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
13:50:49 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
13:50:49 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
13:50:49 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
13:50:49 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
13:50:49 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
13:50:49 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
13:50:49 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
13:50:49 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
13:50:49 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
13:50:49 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
13:50:49 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
13:50:49 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
13:50:49 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
13:50:49 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
13:50:49 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
13:50:49 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
13:50:49 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
13:50:49 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
13:50:49 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
13:50:49 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
13:50:49 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
13:50:49 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
13:50:49 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
13:50:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
13:50:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
13:50:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
13:50:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
13:50:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
13:50:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
13:50:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
13:50:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
13:50:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
13:50:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
13:50:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
13:50:49 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
13:50:49 INFO - load_model: Hyperprior.prior_entropy_model.CDF - torch.Size([64, 1481])
13:50:49 INFO - load_model: Hyperprior.prior_entropy_model.CDF_offset - torch.Size([64])
13:50:49 INFO - load_model: Hyperprior.prior_entropy_model.CDF_length - torch.Size([64])
13:50:49 INFO - load_model: Number of trainable parameters: 181484483
13:50:49 INFO - load_model: Estimated model size (under fp32): 725.938 MB
13:50:49 INFO - load_model: Model init 4.335s
13:50:49 INFO - compress_and_decompress: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/vimeo_med_compression_gan_2022_06_27_17_25/checkpoints', 'crop_size': 256, 'dataset': 'vimeo', 'dataset_path': '/data/videocoding/dnnvc/datasets/Vimeo-90k/tmp/vimeo_septuplet/sequences/', 'discriminator_steps': 1, 'figures_save': 'experiments/vimeo_med_compression_gan_2022_06_27_17_25/figures', 'force_set_gpu': False, 'gan_loss_type': 'non_saturating', 'gpu': 0, 'ignore_schedule': True, 'image_dims': (6, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 1, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_channels_DLMM': 64, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'mixture_components': 4, 'model_mode': 'training', 'model_type': 'compression_gan', 'multigpu': False, 'n_data': 387672, 'n_epochs': 8, 'n_residual_blocks': 9, 'n_steps': 50000, 'name': 'vimeo_med_compression_gan_2022_06_27_17_25', 'noise_dim': 32, 'normalize_input_image': False, 'regime': 'med', 'sample_noise': False, 'save': False, 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/vimeo_med_compression_gan_2022_06_27_17_25', 'storage_save': 'experiments/vimeo_med_compression_gan_2022_06_27_17_25/storage', 'target_rate': 0.3, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/vimeo_med_compression_gan_2022_06_27_17_25/tensorboard', 'timestamp': '2022_06_27_22:37', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': True, 'warmstart_ckpt': 'experiments/vimeo_compression_gan_2022_06_26_20_50/checkpoints/vimeo_compression_gan_2022_06_26_20_50_epoch5_idx161530_2022_06_27_13:23.pt', 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(ckpt_path='experiments/vimeo_med_compression_gan_2022_06_27_17_25/checkpoints/vimeo_med_compression_gan_2022_06_27_17_25_epoch1_idx50001_2022_06_27_22:37.pt', image_dir='uvg', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=True, img_save=False)>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(ckpt_path='experiments/vimeo_med_compression_gan_2022_06_27_17_25/checkpoints/vimeo_med_compression_gan_2022_06_27_17_25_epoch1_idx50001_2022_06_27_22:37.pt', image_dir='uvg', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=True, img_save=False)>, 'ckpt_path': 'experiments/vimeo_med_compression_gan_2022_06_27_17_25/checkpoints/vimeo_med_compression_gan_2022_06_27_17_25_epoch1_idx50001_2022_06_27_22:37.pt', 'image_dir': 'uvg', 'img_save': False, 'metrics': True, 'output_dir': 'data/reconstructions', 'reconstruct': True}
13:50:49 INFO - compress_and_decompress: Building hyperprior probability tables...
13:50:52 INFO - compress_and_decompress: All tables built.
13:50:52 INFO - compress_and_decompress: Starting compression...
14:30:22 INFO - compress_and_decompress: Complete. Reconstructions saved to data/reconstructions. Output statistics saved to data/reconstructions/compression_metrics_med.h5
14:30:22 INFO - compress_and_decompress: Time elapsed: 2369.943 s
14:30:22 INFO - compress_and_decompress: Rate: 1.643 Images / s:
14:30:51 INFO - logger_setup: /home/kumarana/tmp/high_fidel/compress.py
14:30:57 INFO - load_model: Loading model ...
14:30:57 INFO - load_model: MODEL TYPE: compression_gan
14:30:57 INFO - load_model: MODEL MODE: evaluation
14:30:57 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(6, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_7): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_8): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
    (hyperprior_entropy_model): HyperpriorEntropyModel(
      (distribution): HyperpriorDensity()
    )
    (prior_density): PriorDensity()
    (prior_entropy_model): PriorEntropyModel(
      (distribution): PriorDensity()
    )
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
14:30:57 INFO - load_model: Trainable parameters:
14:30:57 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 6, 7, 7])
14:30:57 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
14:30:57 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
14:30:57 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
14:30:57 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
14:30:57 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
14:30:57 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
14:30:57 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
14:30:57 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
14:30:57 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
14:30:57 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
14:30:57 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
14:30:57 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
14:30:57 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
14:30:57 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
14:30:57 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
14:30:57 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
14:30:57 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
14:30:57 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
14:30:57 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
14:30:57 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
14:30:57 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
14:30:57 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
14:30:57 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
14:30:57 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
14:30:57 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
14:30:57 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
14:30:57 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
14:30:57 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
14:30:57 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
14:30:57 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
14:30:57 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
14:30:57 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
14:30:57 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
14:30:57 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
14:30:57 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
14:30:57 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
14:30:57 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_7.conv1.weight - torch.Size([960, 960, 3, 3])
14:30:57 INFO - load_model: Generator.resblock_7.conv1.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.resblock_7.conv2.weight - torch.Size([960, 960, 3, 3])
14:30:57 INFO - load_model: Generator.resblock_7.conv2.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.resblock_7.norm1.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_7.norm1.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_7.norm2.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_7.norm2.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_8.conv1.weight - torch.Size([960, 960, 3, 3])
14:30:57 INFO - load_model: Generator.resblock_8.conv1.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.resblock_8.conv2.weight - torch.Size([960, 960, 3, 3])
14:30:57 INFO - load_model: Generator.resblock_8.conv2.bias - torch.Size([960])
14:30:57 INFO - load_model: Generator.resblock_8.norm1.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_8.norm1.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_8.norm2.gamma - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.resblock_8.norm2.beta - torch.Size([1, 960, 1, 1])
14:30:57 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
14:30:57 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
14:30:57 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
14:30:57 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
14:30:57 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
14:30:57 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
14:30:57 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
14:30:57 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
14:30:57 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
14:30:57 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
14:30:57 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
14:30:57 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
14:30:57 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
14:30:57 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
14:30:57 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
14:30:57 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
14:30:57 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
14:30:57 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
14:30:57 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
14:30:57 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
14:30:57 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
14:30:57 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
14:30:57 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
14:30:57 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
14:30:57 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
14:30:57 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
14:30:57 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
14:30:57 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
14:30:57 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
14:30:57 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
14:30:57 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
14:30:57 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
14:30:57 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
14:30:57 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
14:30:57 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
14:30:57 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
14:30:57 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
14:30:57 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
14:30:57 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
14:30:57 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
14:30:57 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
14:30:57 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
14:30:57 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
14:30:57 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
14:30:57 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
14:30:57 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
14:30:57 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
14:30:57 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
14:30:57 INFO - load_model: Hyperprior.prior_entropy_model.CDF - torch.Size([64, 1481])
14:30:57 INFO - load_model: Hyperprior.prior_entropy_model.CDF_offset - torch.Size([64])
14:30:57 INFO - load_model: Hyperprior.prior_entropy_model.CDF_length - torch.Size([64])
14:30:57 INFO - load_model: Number of trainable parameters: 181484483
14:30:57 INFO - load_model: Estimated model size (under fp32): 725.938 MB
14:30:57 INFO - load_model: Model init 5.720s
14:30:57 INFO - compress_and_decompress: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/vimeo_compression_gan_2022_06_22_20_32/checkpoints', 'crop_size': 256, 'dataset': 'vimeo', 'dataset_path': '/data/videocoding/dnnvc/datasets/Vimeo-90k/tmp/vimeo_septuplet/sequences/', 'discriminator_steps': 1, 'figures_save': 'experiments/vimeo_compression_gan_2022_06_22_20_32/figures', 'force_set_gpu': False, 'gan_loss_type': 'non_saturating', 'gpu': 0, 'ignore_schedule': True, 'image_dims': (6, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 0.5, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_channels_DLMM': 64, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'mixture_components': 4, 'model_mode': 'training', 'model_type': 'compression_gan', 'multigpu': False, 'n_data': 387672, 'n_epochs': 8, 'n_residual_blocks': 9, 'n_steps': 200000, 'name': 'vimeo_compression_gan_2022_06_22_20_32', 'noise_dim': 32, 'normalize_input_image': False, 'regime': 'high', 'sample_noise': False, 'save': False, 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/vimeo_compression_gan_2022_06_22_20_32', 'storage_save': 'experiments/vimeo_compression_gan_2022_06_22_20_32/storage', 'target_rate': 0.45, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/vimeo_compression_gan_2022_06_22_20_32/tensorboard', 'timestamp': '2022_06_23_19:16', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': True, 'warmstart_ckpt': 'experiments/vimeo_compression_2022_06_22_14_49/checkpoints/vimeo_compression_2022_06_22_14_49_epoch1_idx64612_2022_06_22_19:38.pt', 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(ckpt_path='/home/kumarana/tmp/high_fidel/experiments/vimeo_high_compression_gan_2022_06_22_200_32/checkpoints/vimeo_compression_gan_2022_06_22_20_32_epoch6_idx200001_2022_06_23_19:16.pt', image_dir='uvg', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=True, img_save=False)>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(ckpt_path='/home/kumarana/tmp/high_fidel/experiments/vimeo_high_compression_gan_2022_06_22_200_32/checkpoints/vimeo_compression_gan_2022_06_22_20_32_epoch6_idx200001_2022_06_23_19:16.pt', image_dir='uvg', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=True, img_save=False)>, 'ckpt_path': '/home/kumarana/tmp/high_fidel/experiments/vimeo_high_compression_gan_2022_06_22_200_32/checkpoints/vimeo_compression_gan_2022_06_22_20_32_epoch6_idx200001_2022_06_23_19:16.pt', 'image_dir': 'uvg', 'img_save': False, 'metrics': True, 'output_dir': 'data/reconstructions', 'reconstruct': True}
14:30:57 INFO - compress_and_decompress: Building hyperprior probability tables...
14:31:01 INFO - compress_and_decompress: All tables built.
14:31:01 INFO - compress_and_decompress: Starting compression...
15:10:20 INFO - compress_and_decompress: Complete. Reconstructions saved to data/reconstructions. Output statistics saved to data/reconstructions/compression_metrics_high.h5
15:10:20 INFO - compress_and_decompress: Time elapsed: 2358.845 s
15:10:20 INFO - compress_and_decompress: Rate: 1.650 Images / s:
