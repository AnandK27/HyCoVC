15:35:10 INFO - logger_setup: /home/kumarana/tmp/high-fidelity-generative-compression/compress.py
15:35:24 INFO - load_model: Loading model ...
15:35:24 INFO - load_model: MODEL TYPE: compression_gan
15:35:24 INFO - load_model: MODEL MODE: evaluation
15:35:24 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_7): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_8): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
    (hyperprior_entropy_model): HyperpriorEntropyModel(
      (distribution): HyperpriorDensity()
    )
    (prior_density): PriorDensity()
    (prior_entropy_model): PriorEntropyModel(
      (distribution): PriorDensity()
    )
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
15:35:24 INFO - load_model: Trainable parameters:
15:35:24 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])
15:35:24 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
15:35:24 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
15:35:24 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
15:35:24 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
15:35:24 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
15:35:24 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
15:35:24 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
15:35:24 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
15:35:24 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
15:35:24 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
15:35:24 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
15:35:24 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
15:35:24 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
15:35:24 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
15:35:24 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
15:35:24 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
15:35:24 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
15:35:24 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
15:35:24 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
15:35:24 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
15:35:24 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
15:35:24 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
15:35:24 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
15:35:24 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
15:35:24 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
15:35:24 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
15:35:24 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
15:35:24 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
15:35:24 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
15:35:24 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
15:35:24 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
15:35:24 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
15:35:24 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
15:35:24 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
15:35:24 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
15:35:24 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
15:35:24 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_7.conv1.weight - torch.Size([960, 960, 3, 3])
15:35:24 INFO - load_model: Generator.resblock_7.conv1.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.resblock_7.conv2.weight - torch.Size([960, 960, 3, 3])
15:35:24 INFO - load_model: Generator.resblock_7.conv2.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.resblock_7.norm1.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_7.norm1.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_7.norm2.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_7.norm2.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_8.conv1.weight - torch.Size([960, 960, 3, 3])
15:35:24 INFO - load_model: Generator.resblock_8.conv1.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.resblock_8.conv2.weight - torch.Size([960, 960, 3, 3])
15:35:24 INFO - load_model: Generator.resblock_8.conv2.bias - torch.Size([960])
15:35:24 INFO - load_model: Generator.resblock_8.norm1.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_8.norm1.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_8.norm2.gamma - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.resblock_8.norm2.beta - torch.Size([1, 960, 1, 1])
15:35:24 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
15:35:24 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
15:35:24 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
15:35:24 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
15:35:24 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
15:35:24 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
15:35:24 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
15:35:24 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
15:35:24 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
15:35:24 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
15:35:24 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
15:35:24 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
15:35:24 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
15:35:24 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
15:35:24 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
15:35:24 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
15:35:24 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
15:35:24 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
15:35:24 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
15:35:24 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
15:35:24 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
15:35:24 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
15:35:24 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
15:35:24 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
15:35:24 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
15:35:24 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
15:35:24 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
15:35:24 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
15:35:24 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
15:35:24 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
15:35:24 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
15:35:24 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
15:35:24 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
15:35:24 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
15:35:24 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
15:35:24 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
15:35:24 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
15:35:24 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
15:35:24 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
15:35:24 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
15:35:24 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
15:35:24 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
15:35:24 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
15:35:24 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
15:35:24 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
15:35:24 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
15:35:24 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
15:35:24 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
15:35:24 INFO - load_model: Hyperprior.prior_entropy_model.CDF - torch.Size([64, 1481])
15:35:24 INFO - load_model: Hyperprior.prior_entropy_model.CDF_offset - torch.Size([64])
15:35:24 INFO - load_model: Hyperprior.prior_entropy_model.CDF_length - torch.Size([64])
15:35:24 INFO - load_model: Number of trainable parameters: 181475663
15:35:24 INFO - load_model: Estimated model size (under fp32): 725.903 MB
15:35:24 INFO - load_model: Model init 13.293s
15:35:24 INFO - compress_and_decompress: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/flicker_compression_gan_2022_06_09_15_47/checkpoints', 'crop_size': 256, 'dataset': 'flicker', 'dataset_path': '/data/videocoding/dnnvc/datasets/flicker_2W/tmp/flicker_2W_256x256_train_data/', 'discriminator_steps': 1, 'figures_save': 'experiments/flicker_compression_gan_2022_06_09_15_47/figures', 'force_set_gpu': False, 'gan_loss_type': 'non_saturating', 'gpu': 0, 'ignore_schedule': True, 'image_dims': (3, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 2, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_channels_DLMM': 64, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'mixture_components': 4, 'model_mode': 'training', 'model_type': 'compression_gan', 'multigpu': False, 'n_data': 200000, 'n_epochs': 8, 'n_residual_blocks': 9, 'n_steps': 200000, 'name': 'flicker_compression_gan_2022_06_09_15_47', 'noise_dim': 32, 'normalize_input_image': False, 'regime': 'low', 'sample_noise': False, 'save': False, 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/flicker_compression_gan_2022_06_09_15_47', 'storage_save': 'experiments/flicker_compression_gan_2022_06_09_15_47/storage', 'target_rate': 0.14, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/flicker_compression_gan_2022_06_09_15_47/tensorboard', 'timestamp': '2022_06_10_05:36', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': True, 'warmstart_ckpt': 'experiments/flicker_compression_2022_06_09_11_24/checkpoints/flicker_compression_2022_06_09_11_24_epoch2_idx33334_2022_06_09_15:04.pt', 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(ckpt_path='experiments/flicker_compression_gan_2022_06_09_15_47/checkpoints/flicker_compression_gan_2022_06_09_15_47_epoch7_idx133336_2022_06_10_05:36.pt', image_dir='assets/originals', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=False)>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(ckpt_path='experiments/flicker_compression_gan_2022_06_09_15_47/checkpoints/flicker_compression_gan_2022_06_09_15_47_epoch7_idx133336_2022_06_10_05:36.pt', image_dir='assets/originals', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=False)>, 'ckpt_path': 'experiments/flicker_compression_gan_2022_06_09_15_47/checkpoints/flicker_compression_gan_2022_06_09_15_47_epoch7_idx133336_2022_06_10_05:36.pt', 'image_dir': 'assets/originals', 'metrics': False, 'output_dir': 'data/reconstructions', 'reconstruct': True}
15:35:24 INFO - compress_and_decompress: Building hyperprior probability tables...
15:35:31 INFO - compress_and_decompress: All tables built.
15:35:31 INFO - compress_and_decompress: Starting compression...
15:35:58 INFO - compress_and_decompress: Complete. Reconstructions saved to data/reconstructions. Output statistics saved to data/reconstructions/compression_metrics.h5
15:35:58 INFO - compress_and_decompress: Time elapsed: 26.872 s
15:35:58 INFO - compress_and_decompress: Rate: 0.372 Images / s:
